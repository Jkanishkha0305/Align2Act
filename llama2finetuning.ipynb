{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T19:30:20.737842Z",
     "iopub.status.busy": "2025-04-21T19:30:20.737179Z",
     "iopub.status.idle": "2025-04-21T19:30:27.005953Z",
     "shell.execute_reply": "2025-04-21T19:30:27.005267Z",
     "shell.execute_reply.started": "2025-04-21T19:30:20.737818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA2-Accessory'...\n",
      "remote: Enumerating objects: 4047, done.\u001b[K\n",
      "remote: Counting objects: 100% (1350/1350), done.\u001b[K\n",
      "remote: Compressing objects: 100% (476/476), done.\u001b[K\n",
      "remote: Total 4047 (delta 1013), reused 966 (delta 870), pack-reused 2697 (from 1)\u001b[K\n",
      "Receiving objects: 100% (4047/4047), 87.63 MiB | 22.67 MiB/s, done.\n",
      "Resolving deltas: 100% (2563/2563), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Alpha-VLLM/LLaMA2-Accessory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T19:31:34.559001Z",
     "iopub.status.busy": "2025-04-21T19:31:34.558333Z",
     "iopub.status.idle": "2025-04-21T19:31:34.564969Z",
     "shell.execute_reply": "2025-04-21T19:31:34.564406Z",
     "shell.execute_reply.started": "2025-04-21T19:31:34.558974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/LLaMA2-Accessory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA2-Accessory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T19:31:40.699711Z",
     "iopub.status.busy": "2025-04-21T19:31:40.699431Z",
     "iopub.status.idle": "2025-04-21T19:31:48.399308Z",
     "shell.execute_reply": "2025-04-21T19:31:48.398616Z",
     "shell.execute_reply.started": "2025-04-21T19:31:40.699691Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///kaggle/working/LLaMA2-Accessory\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Installing collected packages: LLaMA2-Accessory\n",
      "  Running setup.py develop for LLaMA2-Accessory\n",
      "Successfully installed LLaMA2-Accessory-0.0.1\n"
     ]
    }
   ],
   "source": [
    "# install as package\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:17:30.521360Z",
     "iopub.status.busy": "2025-04-21T21:17:30.520466Z",
     "iopub.status.idle": "2025-04-21T21:17:51.595511Z",
     "shell.execute_reply": "2025-04-21T21:17:51.594529Z",
     "shell.execute_reply.started": "2025-04-21T21:17:30.521326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from flash-attn) (2.5.1+cu124)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp311-cp311-linux_x86_64.whl size=187815463 sha256=d944fc7d2f962bce83fc4708c2fc0c21eaf8255962a0b350ae919362a51b7ef2\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/88/d8/284b89f56af7d5bf366b10d6b8e251ac8a7c7bf3f04203fb4f\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install fairscale\n",
    "!pip install huggingface_hub\n",
    "!pip install sentencepiece\n",
    "!pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T19:48:05.670473Z",
     "iopub.status.busy": "2025-04-21T19:48:05.669648Z",
     "iopub.status.idle": "2025-04-21T19:48:06.060924Z",
     "shell.execute_reply": "2025-04-21T19:48:06.060165Z",
     "shell.execute_reply.started": "2025-04-21T19:48:05.670443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebe727ce22d4a5b89ab4ab9ec7d013e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the `notebook_login` function from the `huggingface_hub` module.\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Execute the `notebook_login` function to log in to the Hugging Face Hub.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T19:59:56.764287Z",
     "iopub.status.busy": "2025-04-21T19:59:56.763570Z",
     "iopub.status.idle": "2025-04-21T20:00:36.745909Z",
     "shell.execute_reply": "2025-04-21T20:00:36.745156Z",
     "shell.execute_reply.started": "2025-04-21T19:59:56.764259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814a74bfaf9c47febab9806d894a9966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/102 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5350a3c531fa449e96a40710570ca44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e597a2e0928a4fe5954619f0d87d9a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.00.pth:   0%|          | 0.00/13.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the necessary function from the huggingface_hub library\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Define the repository information\n",
    "# repo_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "repo_id = \"meta-llama/Llama-2-7b\"\n",
    "# subfolder = \"original\"  # Specify the subfolder within the repository\n",
    "\n",
    "# List of filenames to download\n",
    "filenames = [\"params.json\", \"tokenizer.model\", \"consolidated.00.pth\"] \n",
    "\n",
    "# Specify the directory where you want to save the downloaded files\n",
    "save_directory = \"llama-2-7b/\"  # Replace with your desired path\n",
    "\n",
    "# Download each file\n",
    "for filename in filenames:\n",
    "    hf_hub_download(\n",
    "        repo_id=repo_id,       # Repository ID\n",
    "        filename=filename,     # Name of the file to download\n",
    "        # subfolder=subfolder,   # Subfolder within the repository\n",
    "        local_dir=save_directory  # Directory to save the downloaded file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/meta-llama/Llama-2-7b/resolve/main/params.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:08:23.520908Z",
     "iopub.status.busy": "2025-04-21T20:08:23.520341Z",
     "iopub.status.idle": "2025-04-21T20:08:23.645149Z",
     "shell.execute_reply": "2025-04-21T20:08:23.644434Z",
     "shell.execute_reply.started": "2025-04-21T20:08:23.520883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maccessory\u001b[0m/           \u001b[01;34mLarge-DiT-T2I\u001b[0m/              \u001b[01;34mllama-3-8B\u001b[0m/\n",
      "\u001b[01;34masset\u001b[0m/               LICENSE_llama2              README.md\n",
      "\u001b[01;34mdata_example\u001b[0m/        \u001b[01;34mlight-eval\u001b[0m/                 requirements.txt\n",
      "\u001b[01;34mdocs\u001b[0m/                \u001b[01;34mllama-2-7b\u001b[0m/                 setup.py\n",
      "\u001b[01;34mLarge-DiT-ImageNet\u001b[0m/  \u001b[01;34mLLaMA2_Accessory.egg-info\u001b[0m/  \u001b[01;34mSPHINX\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:09:48.276418Z",
     "iopub.status.busy": "2025-04-21T20:09:48.275821Z",
     "iopub.status.idle": "2025-04-21T20:09:48.415327Z",
     "shell.execute_reply": "2025-04-21T20:09:48.414371Z",
     "shell.execute_reply.started": "2025-04-21T20:09:48.276391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META:\n",
      "  -\n",
      "    path: '../data/alpaca_gpt4_data.json'\n",
      "    type: 'text'"
     ]
    }
   ],
   "source": [
    "cat /kaggle/working/LLaMA2-Accessory/accessory/configs/data/finetune/sg/alpaca.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:13:04.940975Z",
     "iopub.status.busy": "2025-04-21T20:13:04.940545Z",
     "iopub.status.idle": "2025-04-21T20:13:04.947915Z",
     "shell.execute_reply": "2025-04-21T20:13:04.947382Z",
     "shell.execute_reply.started": "2025-04-21T20:13:04.940951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load YAML file\n",
    "file_path = \"/kaggle/working/LLaMA2-Accessory/accessory/configs/data/finetune/sg/alpaca.yaml\"\n",
    "with open(file_path, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# Update path\n",
    "data['META'][0]['path'] = '/kaggle/input/llama2-instruction-finetuning-data/alpaca_gpt4_data.json'\n",
    "\n",
    "# Save back\n",
    "with open(file_path, 'w') as f:\n",
    "    yaml.dump(data, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:13:09.624792Z",
     "iopub.status.busy": "2025-04-21T20:13:09.624077Z",
     "iopub.status.idle": "2025-04-21T20:13:09.743176Z",
     "shell.execute_reply": "2025-04-21T20:13:09.742434Z",
     "shell.execute_reply.started": "2025-04-21T20:13:09.624764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "META:\n",
      "- path: /kaggle/input/llama2-instruction-finetuning-data/alpaca_gpt4_data.json\n",
      "  type: text\n"
     ]
    }
   ],
   "source": [
    "cat /kaggle/working/LLaMA2-Accessory/accessory/configs/data/finetune/sg/alpaca.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:27:43.555045Z",
     "iopub.status.busy": "2025-04-21T20:27:43.554278Z",
     "iopub.status.idle": "2025-04-21T20:27:43.675840Z",
     "shell.execute_reply": "2025-04-21T20:27:43.674817Z",
     "shell.execute_reply.started": "2025-04-21T20:27:43.555015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!/kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh \\\n",
    "  /kaggle/working/LLaMA2-Accessory/llama-2-7b/consolidated.00.pth \\\n",
    "  /kaggle/working/LLaMA2-Accessory/llama-2-7b/params.json \\\n",
    "  /kaggle/working/LLaMA2-Accessory/llama-2-7b/tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:24:59.720932Z",
     "iopub.status.busy": "2025-04-21T20:24:59.720670Z",
     "iopub.status.idle": "2025-04-21T20:24:59.726169Z",
     "shell.execute_reply": "2025-04-21T20:24:59.725570Z",
     "shell.execute_reply.started": "2025-04-21T20:24:59.720912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/LLaMA2-Accessory'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:28:29.250496Z",
     "iopub.status.busy": "2025-04-21T20:28:29.250213Z",
     "iopub.status.idle": "2025-04-21T20:28:29.370622Z",
     "shell.execute_reply": "2025-04-21T20:28:29.369571Z",
     "shell.execute_reply.started": "2025-04-21T20:28:29.250476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!chmod +x /kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:34:24.699682Z",
     "iopub.status.busy": "2025-04-21T20:34:24.699381Z",
     "iopub.status.idle": "2025-04-21T20:34:24.819177Z",
     "shell.execute_reply": "2025-04-21T20:34:24.818151Z",
     "shell.execute_reply.started": "2025-04-21T20:34:24.699658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!chmod +x /kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:32:03.126153Z",
     "iopub.status.busy": "2025-04-21T20:32:03.125840Z",
     "iopub.status.idle": "2025-04-21T20:32:03.246070Z",
     "shell.execute_reply": "2025-04-21T20:32:03.245379Z",
     "shell.execute_reply.started": "2025-04-21T20:32:03.126127Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "pretrained_path=$1\n",
      "pretrained_type=meta_ori\n",
      "llama_config=\"$2 configs/model/finetune/sg/llamaPeft_normBiasLora.json\"\n",
      "tokenizer_path=\"$3\"\n",
      "data_config=configs/data/finetune/sg/alpaca.yaml\n",
      "\n",
      "data_parallel=sdp\n",
      "model_parallel=1\n",
      "\n",
      "exp_name=finetune/sg/alpaca_llamaPeft_normBiasLora\n",
      "echo \"exp name: $exp_name\"\n",
      "mkdir -p output/\"$exp_name\"\n",
      "\n",
      "torchrun --master_port=1112 --nproc_per_node=8 main_finetune.py \\\n",
      "--output_dir output/\"$exp_name\" --epochs 4 --warmup_epochs 1 \\\n",
      "--batch_size 4 --accum_iter 2 --num_workers 4 \\\n",
      "--max_words 512 \\\n",
      "--lr 0.00005 --min_lr 0.000005 --clip_grad 2 --weight_decay 0.02 \\\n",
      "--data_parallel \"$data_parallel\" --model_parallel_size \"$model_parallel\" --checkpointing \\\n",
      "--llama_type llama_peft --llama_config $llama_config --tokenizer_path \"$tokenizer_path\" \\\n",
      "--no_visual \\\n",
      "--pretrained_path \"$pretrained_path\" --pretrained_type=\"$pretrained_type\" \\\n",
      "--data_config $data_config \\\n",
      "2>&1 | tee -a output/\"$exp_name\"/output.log\n",
      "\n",
      "echo \"exp name: $exp_name\""
     ]
    }
   ],
   "source": [
    "cat /kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T20:33:57.447326Z",
     "iopub.status.busy": "2025-04-21T20:33:57.446607Z",
     "iopub.status.idle": "2025-04-21T20:33:57.567415Z",
     "shell.execute_reply": "2025-04-21T20:33:57.566695Z",
     "shell.execute_reply.started": "2025-04-21T20:33:57.447298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"lora_rank\": 16, \"bias_tuning\": true}"
     ]
    }
   ],
   "source": [
    "cat /kaggle/working/LLaMA2-Accessory/accessory/configs/model/finetune/sg/llamaPeft_biasLora.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:13:59.087813Z",
     "iopub.status.busy": "2025-04-21T21:13:59.087349Z",
     "iopub.status.idle": "2025-04-21T21:13:59.094055Z",
     "shell.execute_reply": "2025-04-21T21:13:59.093274Z",
     "shell.execute_reply.started": "2025-04-21T21:13:59.087786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated /kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the new script content\n",
    "new_script_content = \"\"\"#!/bin/bash\n",
    "\n",
    "pretrained_path=$1\n",
    "pretrained_type=meta_ori\n",
    "llama_config=\"$2 configs/model/finetune/sg/llamaPeft_normBiasLora.json\"\n",
    "tokenizer_path=\"$3\"\n",
    "data_config=configs/data/finetune/sg/alpaca.yaml\n",
    "\n",
    "data_parallel=sdp\n",
    "model_parallel=1\n",
    "\n",
    "exp_name=finetune/sg/alpaca_llamaPeft_normBiasLora\n",
    "echo \"exp name: $exp_name\"\n",
    "mkdir -p output/\"$exp_name\"\n",
    "\n",
    "torchrun --master_port=1112 --nproc_per_node=1 /kaggle/working/LLaMA2-Accessory/accessory/main_finetune.py \\\\\n",
    "--output_dir output/\"$exp_name\" --epochs 4 --warmup_epochs 1 \\\\\n",
    "--batch_size 4 --accum_iter 2 --num_workers 4 \\\\\n",
    "--max_words 512 \\\\\n",
    "--lr 0.00005 --min_lr 0.000005 --clip_grad 2 --weight_decay 0.02 \\\\\n",
    "--data_parallel \"$data_parallel\" --model_parallel_size \"$model_parallel\" --checkpointing \\\\\n",
    "--llama_type llama_peft --llama_config $llama_config --tokenizer_path \"$tokenizer_path\" \\\\\n",
    "--no_visual \\\\\n",
    "--pretrained_path \"$pretrained_path\" --pretrained_type=\"$pretrained_type\" \\\\\n",
    "--data_config $data_config \\\\\n",
    "2>&1 | tee -a output/\"$exp_name\"/output.log\n",
    "\n",
    "echo \"exp name: $exp_name\"\n",
    "\"\"\"\n",
    "\n",
    "# Path to your shell script\n",
    "script_path = \"/kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh\"\n",
    "\n",
    "# Write the new content to the file\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(new_script_content)\n",
    "\n",
    "# Make the script executable\n",
    "os.chmod(script_path, 0o755)  # Equivalent to chmod +x\n",
    "\n",
    "print(f\"Successfully updated {script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:14:03.976940Z",
     "iopub.status.busy": "2025-04-21T21:14:03.976367Z",
     "iopub.status.idle": "2025-04-21T21:14:04.108542Z",
     "shell.execute_reply": "2025-04-21T21:14:04.107797Z",
     "shell.execute_reply.started": "2025-04-21T21:14:03.976920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "pretrained_path=$1\n",
      "pretrained_type=meta_ori\n",
      "llama_config=\"$2 configs/model/finetune/sg/llamaPeft_normBiasLora.json\"\n",
      "tokenizer_path=\"$3\"\n",
      "data_config=configs/data/finetune/sg/alpaca.yaml\n",
      "\n",
      "data_parallel=sdp\n",
      "model_parallel=1\n",
      "\n",
      "exp_name=finetune/sg/alpaca_llamaPeft_normBiasLora\n",
      "echo \"exp name: $exp_name\"\n",
      "mkdir -p output/\"$exp_name\"\n",
      "\n",
      "torchrun --master_port=1112 --nproc_per_node=1 /kaggle/working/LLaMA2-Accessory/accessory/main_finetune.py \\\n",
      "--output_dir output/\"$exp_name\" --epochs 4 --warmup_epochs 1 \\\n",
      "--batch_size 4 --accum_iter 2 --num_workers 4 \\\n",
      "--max_words 512 \\\n",
      "--lr 0.00005 --min_lr 0.000005 --clip_grad 2 --weight_decay 0.02 \\\n",
      "--data_parallel \"$data_parallel\" --model_parallel_size \"$model_parallel\" --checkpointing \\\n",
      "--llama_type llama_peft --llama_config $llama_config --tokenizer_path \"$tokenizer_path\" \\\n",
      "--no_visual \\\n",
      "--pretrained_path \"$pretrained_path\" --pretrained_type=\"$pretrained_type\" \\\n",
      "--data_config $data_config \\\n",
      "2>&1 | tee -a output/\"$exp_name\"/output.log\n",
      "\n",
      "echo \"exp name: $exp_name\"\n"
     ]
    }
   ],
   "source": [
    "cat /kaggle/working/LLaMA2-Accessory/accessory/exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:21:50.534102Z",
     "iopub.status.busy": "2025-04-21T21:21:50.533198Z",
     "iopub.status.idle": "2025-04-21T21:22:04.861761Z",
     "shell.execute_reply": "2025-04-21T21:22:04.861058Z",
     "shell.execute_reply.started": "2025-04-21T21:21:50.534059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp name: finetune/sg/alpaca_llamaPeft_normBiasLora\n",
      "2025-04-21 21:21:54.553962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745270514.575591     347 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745270514.581862     347 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/kaggle/working/LLaMA2-Accessory/accessory/main_finetune.py:41: UserWarning: cannot import FusedAdam from apex, use torch AdamW instead\n",
      "  warnings.warn(\"cannot import FusedAdam from apex, use torch AdamW instead\")\n",
      "| distributed init (rank 0): env://, gpu 0\n",
      "[rank0]:[W421 21:22:00.467947749 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\n",
      "[21:22:00.378906] > initializing model parallel with size 1\n",
      "[21:22:00.378961] > initializing ddp with size 1\n",
      "[21:22:00.378973] > initializing pipeline with size 1\n",
      "[21:22:00.381440] job dir: /kaggle/working/LLaMA2-Accessory/accessory\n",
      "[21:22:00.381557] Namespace(batch_size=4,\n",
      "accum_iter=2,\n",
      "llama_type='llama_peft',\n",
      "llama_config=['/kaggle/working/LLaMA2-Accessory/llama-2-7b/params.json',\n",
      "'configs/model/finetune/sg/llamaPeft_normBiasLora.json'],\n",
      "no_visual=True,\n",
      "tokenizer_path='/kaggle/working/LLaMA2-Accessory/llama-2-7b/tokenizer.model',\n",
      "pretrained_path=['/kaggle/working/LLaMA2-Accessory/llama-2-7b/consolidated.00.pth'],\n",
      "pretrained_type='meta_ori',\n",
      "weight_decay=0.02,\n",
      "lr=5e-05,\n",
      "min_lr=5e-06,\n",
      "epochs=4,\n",
      "warmup_epochs=1.0,\n",
      "clip_grad=2,\n",
      "max_words=512,\n",
      "dialog=False,\n",
      "data_config='configs/data/finetune/sg/alpaca.yaml',\n",
      "image_transform='random_resized_crop',\n",
      "cache_ann_on_disk=False,\n",
      "output_dir='output/finetune/sg/alpaca_llamaPeft_normBiasLora',\n",
      "save_interval=1,\n",
      "save_iteration_interval=5000,\n",
      "only_save_trainable=False,\n",
      "seed=0,\n",
      "resume='',\n",
      "num_workers=4,\n",
      "pin_mem=True,\n",
      "dist_on_itp=False,\n",
      "model_parallel_size=1,\n",
      "data_parallel='sdp',\n",
      "precision='bf16',\n",
      "checkpointing=True,\n",
      "quant=False,\n",
      "world_size=1,\n",
      "rank=0,\n",
      "gpu=0,\n",
      "local_rank=0,\n",
      "dist_url='env://',\n",
      "distributed=True,\n",
      "dist_backend='nccl')\n",
      "[21:22:00.382549] Start initialization.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "/kaggle/working/LLaMA2-Accessory/accessory/model/components.py:8: UserWarning: Cannot import apex RMSNorm, switch to vanilla implementation\n",
      "  warnings.warn(\"Cannot import apex RMSNorm, switch to vanilla implementation\")\n",
      "[21:22:01.886301] rope theta: 10000\n",
      "[21:22:01.938652] Model Args:\n",
      " ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=None, vocab_size=32000, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=10000, max_batch_size=32, max_seq_len=512, rope_scaling=None, lora_rank=16, bias_tuning=True, norm_tuning=True)\n",
      "[21:22:01.946901] Model is Peft: True\n",
      "[21:22:01.955462] Trainable parameter count : 41603072 (local rank), 41603072 (all).\n",
      "[21:22:01.955588] Finish initialization.\n",
      "[21:22:01.991965] Params that require gradient:\n",
      "\n",
      "[21:22:01.992031] Param llma.layers.0.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992053] Param llma.layers.0.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992072] Param llma.layers.0.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992090] Param llma.layers.0.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992107] Param llma.layers.0.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992128] Param llma.layers.0.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992166] Param llma.layers.0.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992201] Param llma.layers.0.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992218] Param llma.layers.0.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992259] Param llma.layers.0.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992297] Param llma.layers.0.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992336] Param llma.layers.0.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992373] Param llma.layers.0.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992411] Param llma.layers.0.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992452] Param llma.layers.0.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992490] Param llma.layers.0.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992529] Param llma.layers.0.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992576] Param llma.layers.0.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992608] Param llma.layers.0.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992643] Param llma.layers.0.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992681] Param llma.layers.0.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992720] Param llma.layers.0.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992759] Param llma.layers.0.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992796] Param llma.layers.1.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992835] Param llma.layers.1.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992866] Param llma.layers.1.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992892] Param llma.layers.1.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.992924] Param llma.layers.1.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.992961] Param llma.layers.1.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993003] Param llma.layers.1.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993045] Param llma.layers.1.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993077] Param llma.layers.1.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993103] Param llma.layers.1.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993124] Param llma.layers.1.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993141] Param llma.layers.1.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993158] Param llma.layers.1.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993204] Param llma.layers.1.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993263] Param llma.layers.1.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993302] Param llma.layers.1.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993336] Param llma.layers.1.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993356] Param llma.layers.1.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993374] Param llma.layers.1.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993391] Param llma.layers.1.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993411] Param llma.layers.1.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993442] Param llma.layers.1.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993490] Param llma.layers.1.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993550] Param llma.layers.2.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993582] Param llma.layers.2.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993603] Param llma.layers.2.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993620] Param llma.layers.2.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993637] Param llma.layers.2.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993653] Param llma.layers.2.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993687] Param llma.layers.2.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993730] Param llma.layers.2.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993766] Param llma.layers.2.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993797] Param llma.layers.2.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993818] Param llma.layers.2.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993835] Param llma.layers.2.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993852] Param llma.layers.2.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993869] Param llma.layers.2.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993891] Param llma.layers.2.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993932] Param llma.layers.2.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.993968] Param llma.layers.2.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.993999] Param llma.layers.2.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994022] Param llma.layers.2.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994038] Param llma.layers.2.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994055] Param llma.layers.2.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994071] Param llma.layers.2.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994090] Param llma.layers.2.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994129] Param llma.layers.3.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994164] Param llma.layers.3.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994196] Param llma.layers.3.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994218] Param llma.layers.3.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994267] Param llma.layers.3.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994310] Param llma.layers.3.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994348] Param llma.layers.3.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994379] Param llma.layers.3.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994406] Param llma.layers.3.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994423] Param llma.layers.3.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994440] Param llma.layers.3.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994459] Param llma.layers.3.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994486] Param llma.layers.3.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994529] Param llma.layers.3.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994576] Param llma.layers.3.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994607] Param llma.layers.3.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994633] Param llma.layers.3.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994650] Param llma.layers.3.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994667] Param llma.layers.3.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994684] Param llma.layers.3.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994705] Param llma.layers.3.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994742] Param llma.layers.3.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994785] Param llma.layers.3.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994827] Param llma.layers.4.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994863] Param llma.layers.4.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994893] Param llma.layers.4.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994917] Param llma.layers.4.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994937] Param llma.layers.4.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.994957] Param llma.layers.4.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.994974] Param llma.layers.4.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995003] Param llma.layers.4.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995048] Param llma.layers.4.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995091] Param llma.layers.4.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995131] Param llma.layers.4.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995163] Param llma.layers.4.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995191] Param llma.layers.4.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995237] Param llma.layers.4.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995284] Param llma.layers.4.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995320] Param llma.layers.4.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995352] Param llma.layers.4.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995373] Param llma.layers.4.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995393] Param llma.layers.4.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995412] Param llma.layers.4.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995432] Param llma.layers.4.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995466] Param llma.layers.4.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995506] Param llma.layers.4.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995543] Param llma.layers.5.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995570] Param llma.layers.5.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995590] Param llma.layers.5.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995607] Param llma.layers.5.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995624] Param llma.layers.5.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995648] Param llma.layers.5.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995692] Param llma.layers.5.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995732] Param llma.layers.5.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995764] Param llma.layers.5.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995793] Param llma.layers.5.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995813] Param llma.layers.5.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995833] Param llma.layers.5.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995852] Param llma.layers.5.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995886] Param llma.layers.5.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.995932] Param llma.layers.5.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.995970] Param llma.layers.5.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996001] Param llma.layers.5.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996027] Param llma.layers.5.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996043] Param llma.layers.5.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996060] Param llma.layers.5.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996076] Param llma.layers.5.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996092] Param llma.layers.5.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996133] Param llma.layers.5.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996170] Param llma.layers.6.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996200] Param llma.layers.6.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996224] Param llma.layers.6.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996281] Param llma.layers.6.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996328] Param llma.layers.6.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996364] Param llma.layers.6.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996396] Param llma.layers.6.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996416] Param llma.layers.6.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996436] Param llma.layers.6.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996453] Param llma.layers.6.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996469] Param llma.layers.6.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996496] Param llma.layers.6.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996545] Param llma.layers.6.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996583] Param llma.layers.6.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996613] Param llma.layers.6.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996638] Param llma.layers.6.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996656] Param llma.layers.6.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996672] Param llma.layers.6.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996692] Param llma.layers.6.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996724] Param llma.layers.6.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996765] Param llma.layers.6.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996800] Param llma.layers.6.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996830] Param llma.layers.6.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996853] Param llma.layers.7.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996870] Param llma.layers.7.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996887] Param llma.layers.7.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996903] Param llma.layers.7.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996923] Param llma.layers.7.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.996964] Param llma.layers.7.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.996999] Param llma.layers.7.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997030] Param llma.layers.7.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997049] Param llma.layers.7.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997066] Param llma.layers.7.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997083] Param llma.layers.7.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997100] Param llma.layers.7.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997118] Param llma.layers.7.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997158] Param llma.layers.7.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997193] Param llma.layers.7.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997223] Param llma.layers.7.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997275] Param llma.layers.7.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997305] Param llma.layers.7.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997343] Param llma.layers.7.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997383] Param llma.layers.7.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997415] Param llma.layers.7.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997441] Param llma.layers.7.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997458] Param llma.layers.7.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997475] Param llma.layers.8.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997492] Param llma.layers.8.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997510] Param llma.layers.8.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997550] Param llma.layers.8.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997589] Param llma.layers.8.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997620] Param llma.layers.8.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997645] Param llma.layers.8.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997662] Param llma.layers.8.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997679] Param llma.layers.8.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997698] Param llma.layers.8.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997720] Param llma.layers.8.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997762] Param llma.layers.8.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997799] Param llma.layers.8.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997830] Param llma.layers.8.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997855] Param llma.layers.8.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997872] Param llma.layers.8.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997891] Param llma.layers.8.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997908] Param llma.layers.8.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.997937] Param llma.layers.8.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.997980] Param llma.layers.8.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998019] Param llma.layers.8.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998050] Param llma.layers.8.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998075] Param llma.layers.8.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998092] Param llma.layers.9.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998109] Param llma.layers.9.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998126] Param llma.layers.9.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998145] Param llma.layers.9.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998180] Param llma.layers.9.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998218] Param llma.layers.9.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998262] Param llma.layers.9.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998293] Param llma.layers.9.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998316] Param llma.layers.9.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998335] Param llma.layers.9.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998369] Param llma.layers.9.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998413] Param llma.layers.9.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998449] Param llma.layers.9.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998481] Param llma.layers.9.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998503] Param llma.layers.9.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998522] Param llma.layers.9.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998545] Param llma.layers.9.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998576] Param llma.layers.9.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998616] Param llma.layers.9.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998655] Param llma.layers.9.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998694] Param llma.layers.9.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998732] Param llma.layers.9.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998771] Param llma.layers.9.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998802] Param llma.layers.10.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998837] Param llma.layers.10.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998880] Param llma.layers.10.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998919] Param llma.layers.10.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.998957] Param llma.layers.10.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.998995] Param llma.layers.10.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999025] Param llma.layers.10.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999058] Param llma.layers.10.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999104] Param llma.layers.10.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999141] Param llma.layers.10.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999176] Param llma.layers.10.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999208] Param llma.layers.10.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999254] Param llma.layers.10.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999287] Param llma.layers.10.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999322] Param llma.layers.10.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999360] Param llma.layers.10.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999392] Param llma.layers.10.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999418] Param llma.layers.10.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999435] Param llma.layers.10.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999452] Param llma.layers.10.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999469] Param llma.layers.10.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999486] Param llma.layers.10.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999520] Param llma.layers.10.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999565] Param llma.layers.11.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999596] Param llma.layers.11.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999621] Param llma.layers.11.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999638] Param llma.layers.11.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999655] Param llma.layers.11.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999674] Param llma.layers.11.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999701] Param llma.layers.11.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999743] Param llma.layers.11.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999780] Param llma.layers.11.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999820] Param llma.layers.11.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999862] Param llma.layers.11.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999902] Param llma.layers.11.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:01.999942] Param llma.layers.11.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:01.999985] Param llma.layers.11.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000026] Param llma.layers.11.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000059] Param llma.layers.11.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000087] Param llma.layers.11.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000108] Param llma.layers.11.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000127] Param llma.layers.11.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000145] Param llma.layers.11.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000162] Param llma.layers.11.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000198] Param llma.layers.11.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000255] Param llma.layers.11.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000295] Param llma.layers.12.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000327] Param llma.layers.12.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000354] Param llma.layers.12.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000374] Param llma.layers.12.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000391] Param llma.layers.12.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000408] Param llma.layers.12.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000433] Param llma.layers.12.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000473] Param llma.layers.12.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000511] Param llma.layers.12.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000547] Param llma.layers.12.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000571] Param llma.layers.12.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000589] Param llma.layers.12.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000608] Param llma.layers.12.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000625] Param llma.layers.12.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000651] Param llma.layers.12.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000693] Param llma.layers.12.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000730] Param llma.layers.12.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000761] Param llma.layers.12.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000787] Param llma.layers.12.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000805] Param llma.layers.12.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000823] Param llma.layers.12.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000839] Param llma.layers.12.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000869] Param llma.layers.12.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000909] Param llma.layers.13.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.000945] Param llma.layers.13.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.000976] Param llma.layers.13.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001000] Param llma.layers.13.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001018] Param llma.layers.13.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001035] Param llma.layers.13.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001054] Param llma.layers.13.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001082] Param llma.layers.13.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001124] Param llma.layers.13.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001159] Param llma.layers.13.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001189] Param llma.layers.13.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001214] Param llma.layers.13.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001258] Param llma.layers.13.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001302] Param llma.layers.13.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001336] Param llma.layers.13.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001366] Param llma.layers.13.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001390] Param llma.layers.13.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001407] Param llma.layers.13.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001424] Param llma.layers.13.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001441] Param llma.layers.13.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001462] Param llma.layers.13.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001499] Param llma.layers.13.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001541] Param llma.layers.13.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001571] Param llma.layers.14.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001596] Param llma.layers.14.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001613] Param llma.layers.14.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001629] Param llma.layers.14.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001649] Param llma.layers.14.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001672] Param llma.layers.14.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001713] Param llma.layers.14.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001748] Param llma.layers.14.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001778] Param llma.layers.14.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001803] Param llma.layers.14.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001837] Param llma.layers.14.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001869] Param llma.layers.14.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001911] Param llma.layers.14.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.001951] Param llma.layers.14.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.001982] Param llma.layers.14.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002008] Param llma.layers.14.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002026] Param llma.layers.14.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002043] Param llma.layers.14.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002060] Param llma.layers.14.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002077] Param llma.layers.14.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002111] Param llma.layers.14.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002149] Param llma.layers.14.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002179] Param llma.layers.14.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002205] Param llma.layers.15.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002222] Param llma.layers.15.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002269] Param llma.layers.15.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002306] Param llma.layers.15.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002337] Param llma.layers.15.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002361] Param llma.layers.15.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002378] Param llma.layers.15.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002395] Param llma.layers.15.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002415] Param llma.layers.15.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002438] Param llma.layers.15.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002480] Param llma.layers.15.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002516] Param llma.layers.15.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002551] Param llma.layers.15.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002572] Param llma.layers.15.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002589] Param llma.layers.15.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002608] Param llma.layers.15.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002626] Param llma.layers.15.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002658] Param llma.layers.15.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002702] Param llma.layers.15.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002742] Param llma.layers.15.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002773] Param llma.layers.15.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002800] Param llma.layers.15.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002820] Param llma.layers.15.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002840] Param llma.layers.16.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002857] Param llma.layers.16.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002884] Param llma.layers.16.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002926] Param llma.layers.16.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.002962] Param llma.layers.16.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.002993] Param llma.layers.16.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003015] Param llma.layers.16.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003032] Param llma.layers.16.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003049] Param llma.layers.16.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003066] Param llma.layers.16.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003085] Param llma.layers.16.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003124] Param llma.layers.16.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003160] Param llma.layers.16.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003208] Param llma.layers.16.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003255] Param llma.layers.16.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003301] Param llma.layers.16.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003343] Param llma.layers.16.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003395] Param llma.layers.16.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003432] Param llma.layers.16.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003463] Param llma.layers.16.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003501] Param llma.layers.16.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003564] Param llma.layers.16.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003605] Param llma.layers.16.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003647] Param llma.layers.17.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003683] Param llma.layers.17.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003726] Param llma.layers.17.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003776] Param llma.layers.17.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003811] Param llma.layers.17.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003851] Param llma.layers.17.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003912] Param llma.layers.17.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.003956] Param llma.layers.17.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.003997] Param llma.layers.17.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004027] Param llma.layers.17.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004054] Param llma.layers.17.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004073] Param llma.layers.17.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004091] Param llma.layers.17.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004109] Param llma.layers.17.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004134] Param llma.layers.17.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004180] Param llma.layers.17.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004218] Param llma.layers.17.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004264] Param llma.layers.17.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004298] Param llma.layers.17.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004320] Param llma.layers.17.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004338] Param llma.layers.17.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004377] Param llma.layers.17.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004427] Param llma.layers.17.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004458] Param llma.layers.18.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004483] Param llma.layers.18.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004500] Param llma.layers.18.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004517] Param llma.layers.18.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004558] Param llma.layers.18.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004595] Param llma.layers.18.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004634] Param llma.layers.18.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004665] Param llma.layers.18.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004691] Param llma.layers.18.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004708] Param llma.layers.18.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004725] Param llma.layers.18.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004742] Param llma.layers.18.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004758] Param llma.layers.18.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004790] Param llma.layers.18.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004832] Param llma.layers.18.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004870] Param llma.layers.18.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004902] Param llma.layers.18.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004926] Param llma.layers.18.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004944] Param llma.layers.18.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004960] Param llma.layers.18.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.004978] Param llma.layers.18.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.004996] Param llma.layers.18.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005031] Param llma.layers.18.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005067] Param llma.layers.19.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005097] Param llma.layers.19.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005123] Param llma.layers.19.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005140] Param llma.layers.19.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005157] Param llma.layers.19.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005173] Param llma.layers.19.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005193] Param llma.layers.19.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005247] Param llma.layers.19.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005288] Param llma.layers.19.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005318] Param llma.layers.19.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005343] Param llma.layers.19.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005360] Param llma.layers.19.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005377] Param llma.layers.19.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005394] Param llma.layers.19.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005419] Param llma.layers.19.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005463] Param llma.layers.19.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005499] Param llma.layers.19.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005529] Param llma.layers.19.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005587] Param llma.layers.19.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005627] Param llma.layers.19.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005668] Param llma.layers.19.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005704] Param llma.layers.19.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005734] Param llma.layers.19.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005758] Param llma.layers.20.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005776] Param llma.layers.20.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005795] Param llma.layers.20.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005812] Param llma.layers.20.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005835] Param llma.layers.20.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005876] Param llma.layers.20.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005911] Param llma.layers.20.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005942] Param llma.layers.20.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.005966] Param llma.layers.20.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.005985] Param llma.layers.20.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006005] Param llma.layers.20.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006024] Param llma.layers.20.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006055] Param llma.layers.20.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006100] Param llma.layers.20.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006142] Param llma.layers.20.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006177] Param llma.layers.20.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006213] Param llma.layers.20.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006262] Param llma.layers.20.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006301] Param llma.layers.20.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006332] Param llma.layers.20.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006358] Param llma.layers.20.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006375] Param llma.layers.20.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006392] Param llma.layers.20.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006408] Param llma.layers.21.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006429] Param llma.layers.21.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006469] Param llma.layers.21.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006505] Param llma.layers.21.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006543] Param llma.layers.21.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006564] Param llma.layers.21.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006583] Param llma.layers.21.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006603] Param llma.layers.21.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006620] Param llma.layers.21.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006655] Param llma.layers.21.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006701] Param llma.layers.21.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006744] Param llma.layers.21.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006781] Param llma.layers.21.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006812] Param llma.layers.21.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006841] Param llma.layers.21.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006861] Param llma.layers.21.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006898] Param llma.layers.21.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006935] Param llma.layers.21.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.006965] Param llma.layers.21.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.006989] Param llma.layers.21.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007006] Param llma.layers.21.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007023] Param llma.layers.21.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007039] Param llma.layers.21.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007064] Param llma.layers.22.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007104] Param llma.layers.22.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007140] Param llma.layers.22.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007169] Param llma.layers.22.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007193] Param llma.layers.22.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007210] Param llma.layers.22.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007248] Param llma.layers.22.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007292] Param llma.layers.22.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007326] Param llma.layers.22.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007356] Param llma.layers.22.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007380] Param llma.layers.22.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007397] Param llma.layers.22.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007416] Param llma.layers.22.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007435] Param llma.layers.22.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007467] Param llma.layers.22.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007510] Param llma.layers.22.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007550] Param llma.layers.22.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007580] Param llma.layers.22.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007599] Param llma.layers.22.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007619] Param llma.layers.22.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007636] Param llma.layers.22.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007653] Param llma.layers.22.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007683] Param llma.layers.22.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007727] Param llma.layers.23.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007770] Param llma.layers.23.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007808] Param llma.layers.23.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007839] Param llma.layers.23.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007867] Param llma.layers.23.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007889] Param llma.layers.23.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007929] Param llma.layers.23.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.007966] Param llma.layers.23.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.007996] Param llma.layers.23.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008020] Param llma.layers.23.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008037] Param llma.layers.23.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008057] Param llma.layers.23.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008076] Param llma.layers.23.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008103] Param llma.layers.23.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008146] Param llma.layers.23.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008181] Param llma.layers.23.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008211] Param llma.layers.23.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008254] Param llma.layers.23.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008286] Param llma.layers.23.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008322] Param llma.layers.23.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008359] Param llma.layers.23.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008390] Param llma.layers.23.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008415] Param llma.layers.23.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008432] Param llma.layers.24.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008449] Param llma.layers.24.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008466] Param llma.layers.24.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008483] Param llma.layers.24.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008517] Param llma.layers.24.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008560] Param llma.layers.24.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008590] Param llma.layers.24.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008615] Param llma.layers.24.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008632] Param llma.layers.24.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008649] Param llma.layers.24.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008668] Param llma.layers.24.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008699] Param llma.layers.24.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008742] Param llma.layers.24.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008780] Param llma.layers.24.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008811] Param llma.layers.24.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008836] Param llma.layers.24.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008853] Param llma.layers.24.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008870] Param llma.layers.24.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008887] Param llma.layers.24.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008904] Param llma.layers.24.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.008946] Param llma.layers.24.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.008982] Param llma.layers.24.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009015] Param llma.layers.24.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009036] Param llma.layers.25.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009055] Param llma.layers.25.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009075] Param llma.layers.25.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009093] Param llma.layers.25.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009122] Param llma.layers.25.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009165] Param llma.layers.25.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009199] Param llma.layers.25.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009248] Param llma.layers.25.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009283] Param llma.layers.25.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009313] Param llma.layers.25.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009351] Param llma.layers.25.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009389] Param llma.layers.25.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009419] Param llma.layers.25.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009445] Param llma.layers.25.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009462] Param llma.layers.25.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009480] Param llma.layers.25.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009497] Param llma.layers.25.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009516] Param llma.layers.25.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009565] Param llma.layers.25.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009602] Param llma.layers.25.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009634] Param llma.layers.25.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009655] Param llma.layers.25.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009671] Param llma.layers.25.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009691] Param llma.layers.26.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009708] Param llma.layers.26.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009739] Param llma.layers.26.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009782] Param llma.layers.26.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009820] Param llma.layers.26.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009851] Param llma.layers.26.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009877] Param llma.layers.26.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009894] Param llma.layers.26.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009912] Param llma.layers.26.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009929] Param llma.layers.26.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.009945] Param llma.layers.26.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.009983] Param llma.layers.26.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010021] Param llma.layers.26.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010053] Param llma.layers.26.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010078] Param llma.layers.26.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010095] Param llma.layers.26.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010112] Param llma.layers.26.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010129] Param llma.layers.26.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010149] Param llma.layers.26.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010190] Param llma.layers.26.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010226] Param llma.layers.26.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010272] Param llma.layers.26.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010302] Param llma.layers.26.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010323] Param llma.layers.27.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010340] Param llma.layers.27.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010374] Param llma.layers.27.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010419] Param llma.layers.27.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010460] Param llma.layers.27.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010491] Param llma.layers.27.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010518] Param llma.layers.27.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010540] Param llma.layers.27.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010558] Param llma.layers.27.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010574] Param llma.layers.27.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010597] Param llma.layers.27.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010639] Param llma.layers.27.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010674] Param llma.layers.27.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010704] Param llma.layers.27.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010729] Param llma.layers.27.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010746] Param llma.layers.27.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010763] Param llma.layers.27.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010782] Param llma.layers.27.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010817] Param llma.layers.27.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010861] Param llma.layers.27.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010897] Param llma.layers.27.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010927] Param llma.layers.27.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010951] Param llma.layers.27.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.010968] Param llma.layers.28.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.010984] Param llma.layers.28.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011001] Param llma.layers.28.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011025] Param llma.layers.28.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011068] Param llma.layers.28.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011104] Param llma.layers.28.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011134] Param llma.layers.28.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011159] Param llma.layers.28.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011176] Param llma.layers.28.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011193] Param llma.layers.28.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011210] Param llma.layers.28.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011245] Param llma.layers.28.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011291] Param llma.layers.28.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011329] Param llma.layers.28.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011361] Param llma.layers.28.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011385] Param llma.layers.28.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011402] Param llma.layers.28.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011419] Param llma.layers.28.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011436] Param llma.layers.28.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011462] Param llma.layers.28.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011506] Param llma.layers.28.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011560] Param llma.layers.28.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011597] Param llma.layers.28.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011629] Param llma.layers.29.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011651] Param llma.layers.29.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011671] Param llma.layers.29.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011688] Param llma.layers.29.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011705] Param llma.layers.29.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011733] Param llma.layers.29.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011775] Param llma.layers.29.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011812] Param llma.layers.29.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011843] Param llma.layers.29.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011868] Param llma.layers.29.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011885] Param llma.layers.29.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011902] Param llma.layers.29.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011921] Param llma.layers.29.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.011954] Param llma.layers.29.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.011998] Param llma.layers.29.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012034] Param llma.layers.29.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012067] Param llma.layers.29.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012089] Param llma.layers.29.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012108] Param llma.layers.29.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012127] Param llma.layers.29.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012147] Param llma.layers.29.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012178] Param llma.layers.29.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012222] Param llma.layers.29.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012271] Param llma.layers.30.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012304] Param llma.layers.30.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012326] Param llma.layers.30.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012343] Param llma.layers.30.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012360] Param llma.layers.30.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012380] Param llma.layers.30.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012412] Param llma.layers.30.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012454] Param llma.layers.30.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012490] Param llma.layers.30.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012520] Param llma.layers.30.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012548] Param llma.layers.30.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012566] Param llma.layers.30.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012583] Param llma.layers.30.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012601] Param llma.layers.30.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012630] Param llma.layers.30.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012673] Param llma.layers.30.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012711] Param llma.layers.30.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012741] Param llma.layers.30.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012764] Param llma.layers.30.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012781] Param llma.layers.30.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012803] Param llma.layers.30.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012820] Param llma.layers.30.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012838] Param llma.layers.30.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012877] Param llma.layers.31.attention.wq.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012912] Param llma.layers.31.attention.wq.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.012944] Param llma.layers.31.attention.wq.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012966] Param llma.layers.31.attention.wk.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.012984] Param llma.layers.31.attention.wk.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013001] Param llma.layers.31.attention.wk.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013020] Param llma.layers.31.attention.wv.bias: requires_grad True, local_size torch.Size([4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013047] Param llma.layers.31.attention.wv.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013093] Param llma.layers.31.attention.wv.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013131] Param llma.layers.31.attention.wo.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013162] Param llma.layers.31.attention.wo.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013187] Param llma.layers.31.attention.wo.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013204] Param llma.layers.31.feed_forward.w1.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013221] Param llma.layers.31.feed_forward.w1.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013268] Param llma.layers.31.feed_forward.w1.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013306] Param llma.layers.31.feed_forward.w2.bias: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013337] Param llma.layers.31.feed_forward.w2.lora_a.weight: requires_grad True, local_size torch.Size([16, 11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013362] Param llma.layers.31.feed_forward.w2.lora_b.weight: requires_grad True, local_size torch.Size([4096, 16]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013379] Param llma.layers.31.feed_forward.w3.bias: requires_grad True, local_size torch.Size([11008]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013396] Param llma.layers.31.feed_forward.w3.lora_a.weight: requires_grad True, local_size torch.Size([16, 4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013413] Param llma.layers.31.feed_forward.w3.lora_b.weight: requires_grad True, local_size torch.Size([11008, 16]), model_parallel True, dtype torch.float32\n",
      "[21:22:02.013431] Param llma.layers.31.attention_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013476] Param llma.layers.31.ffn_norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013521] Param llma.norm.weight: requires_grad True, local_size torch.Size([4096]), model_parallel False, dtype torch.float32\n",
      "[21:22:02.013551] \n",
      "Params that do not require gradient:\n",
      "\n",
      "[21:22:02.013593] Param llma.tok_embeddings.weight: requires_grad False, local_size torch.Size([32000, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013629] Param llma.layers.0.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013660] Param llma.layers.0.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013681] Param llma.layers.0.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013701] Param llma.layers.0.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013720] Param llma.layers.0.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013741] Param llma.layers.0.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013780] Param llma.layers.0.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013821] Param llma.layers.1.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013871] Param llma.layers.1.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013894] Param llma.layers.1.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013912] Param llma.layers.1.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013931] Param llma.layers.1.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013949] Param llma.layers.1.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.013984] Param llma.layers.1.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014037] Param llma.layers.2.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014068] Param llma.layers.2.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014095] Param llma.layers.2.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014112] Param llma.layers.2.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014130] Param llma.layers.2.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014147] Param llma.layers.2.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014173] Param llma.layers.2.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014216] Param llma.layers.3.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014267] Param llma.layers.3.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014298] Param llma.layers.3.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014322] Param llma.layers.3.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014341] Param llma.layers.3.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014358] Param llma.layers.3.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014382] Param llma.layers.3.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014424] Param llma.layers.4.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014461] Param llma.layers.4.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014492] Param llma.layers.4.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014516] Param llma.layers.4.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014538] Param llma.layers.4.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014559] Param llma.layers.4.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014579] Param llma.layers.4.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014612] Param llma.layers.5.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014652] Param llma.layers.5.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014684] Param llma.layers.5.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014711] Param llma.layers.5.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014728] Param llma.layers.5.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014746] Param llma.layers.5.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014763] Param llma.layers.5.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014781] Param llma.layers.6.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014825] Param llma.layers.6.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014877] Param llma.layers.6.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014919] Param llma.layers.6.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014960] Param llma.layers.6.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.014992] Param llma.layers.6.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015020] Param llma.layers.6.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015038] Param llma.layers.7.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015056] Param llma.layers.7.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015073] Param llma.layers.7.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015093] Param llma.layers.7.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015133] Param llma.layers.7.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015169] Param llma.layers.7.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015202] Param llma.layers.7.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015225] Param llma.layers.8.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015285] Param llma.layers.8.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015321] Param llma.layers.8.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015351] Param llma.layers.8.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015376] Param llma.layers.8.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015393] Param llma.layers.8.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015411] Param llma.layers.8.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015428] Param llma.layers.9.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015453] Param llma.layers.9.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015497] Param llma.layers.9.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015542] Param llma.layers.9.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015574] Param llma.layers.9.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015600] Param llma.layers.9.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015620] Param llma.layers.9.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015639] Param llma.layers.10.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015657] Param llma.layers.10.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015686] Param llma.layers.10.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015731] Param llma.layers.10.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015769] Param llma.layers.10.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015800] Param llma.layers.10.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015826] Param llma.layers.10.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015844] Param llma.layers.11.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015861] Param llma.layers.11.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015878] Param llma.layers.11.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015902] Param llma.layers.11.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015940] Param llma.layers.11.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.015978] Param llma.layers.11.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016010] Param llma.layers.11.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016035] Param llma.layers.12.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016053] Param llma.layers.12.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016070] Param llma.layers.12.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016090] Param llma.layers.12.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016121] Param llma.layers.12.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016166] Param llma.layers.12.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016201] Param llma.layers.12.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016250] Param llma.layers.13.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016283] Param llma.layers.13.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016311] Param llma.layers.13.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016347] Param llma.layers.13.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016387] Param llma.layers.13.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016419] Param llma.layers.13.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016445] Param llma.layers.13.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016465] Param llma.layers.14.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016485] Param llma.layers.14.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016507] Param llma.layers.14.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016547] Param llma.layers.14.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016587] Param llma.layers.14.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016619] Param llma.layers.14.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016645] Param llma.layers.14.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016663] Param llma.layers.15.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016680] Param llma.layers.15.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016697] Param llma.layers.15.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016721] Param llma.layers.15.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016762] Param llma.layers.15.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016798] Param llma.layers.15.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016830] Param llma.layers.15.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016851] Param llma.layers.16.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016870] Param llma.layers.16.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016890] Param llma.layers.16.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016909] Param llma.layers.16.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016946] Param llma.layers.16.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.016985] Param llma.layers.16.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017017] Param llma.layers.16.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017043] Param llma.layers.17.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017061] Param llma.layers.17.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017078] Param llma.layers.17.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017095] Param llma.layers.17.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017114] Param llma.layers.17.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017155] Param llma.layers.17.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017194] Param llma.layers.17.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017224] Param llma.layers.18.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017273] Param llma.layers.18.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017304] Param llma.layers.18.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017331] Param llma.layers.18.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017372] Param llma.layers.18.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017409] Param llma.layers.18.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017439] Param llma.layers.18.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017463] Param llma.layers.19.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017480] Param llma.layers.19.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017500] Param llma.layers.19.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017517] Param llma.layers.19.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017554] Param llma.layers.19.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017594] Param llma.layers.19.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017625] Param llma.layers.19.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017652] Param llma.layers.20.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017670] Param llma.layers.20.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017687] Param llma.layers.20.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017704] Param llma.layers.20.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017720] Param llma.layers.20.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017753] Param llma.layers.20.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017798] Param llma.layers.20.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017837] Param llma.layers.21.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017869] Param llma.layers.21.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017896] Param llma.layers.21.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017914] Param llma.layers.21.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017931] Param llma.layers.21.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017949] Param llma.layers.21.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.017967] Param llma.layers.21.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018001] Param llma.layers.22.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018039] Param llma.layers.22.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018071] Param llma.layers.22.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018096] Param llma.layers.22.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018114] Param llma.layers.22.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018130] Param llma.layers.22.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018148] Param llma.layers.22.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018169] Param llma.layers.23.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018207] Param llma.layers.23.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018258] Param llma.layers.23.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018290] Param llma.layers.23.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018316] Param llma.layers.23.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018334] Param llma.layers.23.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018351] Param llma.layers.23.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018375] Param llma.layers.24.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018420] Param llma.layers.24.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018459] Param llma.layers.24.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018491] Param llma.layers.24.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018516] Param llma.layers.24.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018539] Param llma.layers.24.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018559] Param llma.layers.24.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018579] Param llma.layers.25.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018610] Param llma.layers.25.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018655] Param llma.layers.25.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018691] Param llma.layers.25.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018723] Param llma.layers.25.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018745] Param llma.layers.25.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018763] Param llma.layers.25.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018783] Param llma.layers.26.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018805] Param llma.layers.26.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018838] Param llma.layers.26.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018879] Param llma.layers.26.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018910] Param llma.layers.26.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018937] Param llma.layers.26.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018954] Param llma.layers.26.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018972] Param llma.layers.27.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.018989] Param llma.layers.27.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019007] Param llma.layers.27.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019039] Param llma.layers.27.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019078] Param llma.layers.27.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019110] Param llma.layers.27.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019137] Param llma.layers.27.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019154] Param llma.layers.28.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019172] Param llma.layers.28.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019189] Param llma.layers.28.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019211] Param llma.layers.28.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019267] Param llma.layers.28.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019304] Param llma.layers.28.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019336] Param llma.layers.28.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019358] Param llma.layers.29.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019376] Param llma.layers.29.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019393] Param llma.layers.29.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019413] Param llma.layers.29.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019447] Param llma.layers.29.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019492] Param llma.layers.29.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019528] Param llma.layers.29.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019565] Param llma.layers.30.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019587] Param llma.layers.30.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019604] Param llma.layers.30.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019621] Param llma.layers.30.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019638] Param llma.layers.30.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019672] Param llma.layers.30.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019716] Param llma.layers.30.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019751] Param llma.layers.31.attention.wq.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019783] Param llma.layers.31.attention.wk.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019806] Param llma.layers.31.attention.wv.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019826] Param llma.layers.31.attention.wo.weight: requires_grad False, local_size torch.Size([4096, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019846] Param llma.layers.31.feed_forward.w1.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019866] Param llma.layers.31.feed_forward.w2.weight: requires_grad False, local_size torch.Size([4096, 11008]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019900] Param llma.layers.31.feed_forward.w3.weight: requires_grad False, local_size torch.Size([11008, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.019940] Param llma.output.weight: requires_grad False, local_size torch.Size([32000, 4096]), model_parallel True, dtype torch.bfloat16\n",
      "[21:22:02.020037] load pretrained from ['/kaggle/working/LLaMA2-Accessory/llama-2-7b/consolidated.00.pth']\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/kaggle/working/LLaMA2-Accessory/accessory/main_finetune.py\", line 369, in <module>\n",
      "[rank0]:     main(args)\n",
      "[rank0]:   File \"/kaggle/working/LLaMA2-Accessory/accessory/main_finetune.py\", line 221, in main\n",
      "[rank0]:     load_result = load_tensor_parallel_model_list(model, args.pretrained_path)\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/kaggle/working/LLaMA2-Accessory/accessory/util/tensor_parallel.py\", line 460, in load_tensor_parallel_model_list\n",
      "[rank0]:     inferred_format, _ = infer_checkpoint_format_and_mp_size(path)\n",
      "[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/kaggle/working/LLaMA2-Accessory/accessory/util/tensor_parallel.py\", line 351, in infer_checkpoint_format_and_mp_size\n",
      "[rank0]:     raise NotImplementedError(\"The given path does not point to a valid \"\n",
      "[rank0]: NotImplementedError: The given path does not point to a valid folder.\n",
      "[rank0]:[W421 21:22:02.149548685 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "E0421 21:22:04.425000 344 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 347) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 919, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 910, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/kaggle/working/LLaMA2-Accessory/accessory/main_finetune.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-04-21_21:22:04\n",
      "  host      : f69791fa203e\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 347)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "exp name: finetune/sg/alpaca_llamaPeft_normBiasLora\n"
     ]
    }
   ],
   "source": [
    "!exps/finetune/sg/alpaca_llamaPeft_normBiasLora.sh \\\n",
    "  '/kaggle/working/LLaMA2-Accessory/llama-2-7b/consolidated.00.pth' \\\n",
    "  '/kaggle/working/LLaMA2-Accessory/llama-2-7b/params.json' \\\n",
    "  '/kaggle/working/LLaMA2-Accessory/llama-2-7b/tokenizer.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:03:32.811369Z",
     "iopub.status.busy": "2025-04-21T21:03:32.810556Z",
     "iopub.status.idle": "2025-04-21T21:03:32.816560Z",
     "shell.execute_reply": "2025-04-21T21:03:32.815869Z",
     "shell.execute_reply.started": "2025-04-21T21:03:32.811340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/LLaMA2-Accessory/accessory'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T21:11:31.066140Z",
     "iopub.status.busy": "2025-04-21T21:11:31.065824Z",
     "iopub.status.idle": "2025-04-21T21:11:32.641875Z",
     "shell.execute_reply": "2025-04-21T21:11:32.641175Z",
     "shell.execute_reply.started": "2025-04-21T21:11:31.066115Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())  # Should match what you expect"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7213977,
     "sourceId": 11505719,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
